<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chatbot mit Gemini (Glitch WebSocket)</title>
  <style>
    body { background-color: #000; color: #fff; font-family: Arial, sans-serif; }
    #chatbox { width: 100%; height: 400px; overflow-y: scroll; border: 1px solid #fff; padding: 10px; background-color: #000; color: #fff; }
    #onAir { color: red; font-size: 1.5em; display: none; }
    #recordButton { margin-top: 10px; padding: 10px; background-color: green; color: white; border: none; cursor: pointer; }
    #recordButton.stop { background-color: red; }
    #browserMessages { margin-top: 10px; padding: 10px; border: 1px solid #fff; background-color: #000; color: #fff; }
  </style>
</head>
<body>
  <div id="onAir">On Air</div>
  <div id="chatbox"></div>
  <button id="recordButton">Verbindung starten</button>
  <div id="browserMessages"></div>
  <script>
    const WS_URL = 'https://palettix-socket.glitch.me/'; // Ersetzen Sie mit Ihrer Glitch-URL
    let ws;
    let audioContext;
    let recorder;
    let stream;
    const recordButton = document.getElementById('recordButton');
    const chatbox = document.getElementById('chatbox');
    const onAir = document.getElementById('onAir');
    const browserMessages = document.getElementById('browserMessages');

    const processorCode = `
      class RecorderProcessor extends AudioWorkletProcessor {
        constructor() {
          super();
          this.buffer = [];
          this.speechThreshold = 0.05;
          this.silentThreshold = 200; // 2 Sekunden Stille
          this.minBufferLength = 48000; // 3 Sekunden bei 16 kHz
          this.silentCount = 0;
          this.isSpeaking = false;
        }

        process(inputs) {
          const input = inputs[0];
          if (input.length > 0) {
            const samples = input[0];
            const rms = Math.sqrt(samples.reduce((sum, val) => sum + val * val, 0) / samples.length);

            if (rms > this.speechThreshold) {
              this.buffer.push(...samples);
              this.silentCount = 0;
              if (!this.isSpeaking) {
                this.isSpeaking = true;
                this.port.postMessage({ eventType: 'speechStart' });
              }
            } else if (this.isSpeaking) {
              this.buffer.push(...samples);
              this.silentCount++;
              if (this.silentCount >= this.silentThreshold && this.buffer.length >= this.minBufferLength) {
                this.isSpeaking = false;
                this.port.postMessage({ eventType: 'speechEnd', audioBuffer: new Float32Array(this.buffer) });
                this.buffer = [];
                this.silentCount = 0;
              }
            }
          }
          return true;
        }
      }
      registerProcessor('recorder-processor', RecorderProcessor);
    `;

    async function loadAudioWorklet() {
      audioContext = new AudioContext({ sampleRate: 16000 });
      const blob = new Blob([processorCode], { type: 'application/javascript' });
      const blobURL = URL.createObjectURL(blob);
      await audioContext.audioWorklet.addModule(blobURL);
    }

    recordButton.addEventListener('click', async () => {
      if (!recorder) {
        if (!audioContext) await loadAudioWorklet();
        startRecording();
      } else {
        stopRecording();
      }
    });

    function startRecording() {
      ws = new WebSocket(WS_URL);
      
      ws.onopen = () => {
        console.log('WebSocket verbunden');
        navigator.mediaDevices.getUserMedia({ audio: true })
          .then(s => {
            stream = s;
            const input = audioContext.createMediaStreamSource(stream);
            recorder = new AudioWorkletNode(audioContext, 'recorder-processor');
            input.connect(recorder);

            recorder.port.onmessage = (e) => {
              if (e.data.eventType === 'speechStart') {
                browserMessages.innerHTML += `<p>Sprache erkannt</p>`;
              } else if (e.data.eventType === 'speechEnd') {
                sendAudioChunk(e.data.audioBuffer);
              }
            };
            recordButton.textContent = 'Verbindung stoppen';
            recordButton.classList.add('stop');
            onAir.style.display = 'block';

            // Ping alle 60 Sekunden, um den Server wach zu halten
            setInterval(() => {
              if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: 'ping' }));
              }
            }, 60000);
          })
          .catch(err => {
            console.error('Mikrofonzugriff fehlgeschlagen:', err);
            browserMessages.innerHTML += `<p>Fehler: Mikrofonzugriff fehlgeschlagen - ${err.message}</p>`;
          });
      };

      ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        if (data.type === 'text') {
          chatbox.innerHTML += `<p>Bot: ${data.data}</p>`;
          speak(data.data);
        } else if (data.type === 'error') {
          browserMessages.innerHTML += `<p>Fehler: ${data.message}</p>`;
        } else if (data.type === 'pong') {
          console.log('Pong empfangen');
        }
      };

      ws.onerror = (error) => {
        console.error('WebSocket-Fehler:', error);
        browserMessages.innerHTML += `<p>WebSocket-Fehler</p>`;
      };

      ws.onclose = () => {
        console.log('WebSocket geschlossen');
        stopRecording();
      };
    }

    function stopRecording() {
      if (recorder) {
        recorder.port.postMessage('stop');
        recorder.disconnect();
        stream.getTracks().forEach(track => track.stop());
      }
      if (ws) ws.close();
      recordButton.textContent = 'Verbindung starten';
      recordButton.classList.remove('stop');
      onAir.style.display = 'none';
      recorder = null;
    }

    function sendAudioChunk(audioBuffer) {
      const samples = audioBuffer;
      if (!samples || samples.length === 0) {
        browserMessages.innerHTML += `<p>Fehler: Keine Audiodaten vorhanden</p>`;
        return;
      }

      const wavBlob = encodeWAV(samples, 16000);
      const reader = new FileReader();
      reader.readAsDataURL(wavBlob);
      reader.onloadend = () => {
        const base64Audio = reader.result.split(',')[1];
        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: 'audio', chunk: base64Audio }));
        }
      };
    }

    function encodeWAV(samples, sampleRate) {
      const buffer = new ArrayBuffer(44 + samples.length * 2);
      const view = new DataView(buffer);

      writeString(view, 0, 'RIFF');
      view.setUint32(4, 36 + samples.length * 2, true);
      writeString(view, 8, 'WAVE');
      writeString(view, 12, 'fmt ');
      view.setUint32(16, 16, true);
      view.setUint16(20, 1, true);
      view.setUint16(22, 1, true);
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, sampleRate * 2, true);
      view.setUint16(32, 2, true);
      view.setUint16(34, 16, true);
      writeString(view, 36, 'data');
      view.setUint32(40, samples.length * 2, true);

      let offset = 44;
      samples.forEach(sample => {
        const s = Math.max(-1, Math.min(1, sample));
        view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
        offset += 2;
      });

      return new Blob([view], { type: 'audio/wav' });
    }

    function writeString(view, offset, string) {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    }

    function speak(text) {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = 'de-DE';
      window.speechSynthesis.speak(utterance);
    }
  </script>
</body>
</html>
